# 理论探讨：Mneme 神经动力学架构 (液体神经网络与赫布学习)

> **已正式纳入**: [ADR-016](MANIFESTO.md)（液体时间常数网络）、[ADR-017](MANIFESTO.md)（赫布可塑性）、[ADR-018](MANIFESTO.md)（躯体解码器）

## 1. 当前大模型范式的核心局限
目前业界完全依赖大型语言模型 (LLM) 来模拟“人格”。这在根基上是存在缺陷的，因为 LLM 本身是无维度、无时间概念且无内部状态的。它们本质上只是预测下一个 Token 的马尔可夫链。如果系统只能完全依赖 Prompt（提示词）告诉模型：“你现在很生气”，这实际上就把人工智能降级成了一个按剧本演戏的演员，这充其量叫“提示词工程” (Prompt Engineering)，而不是在创造生命。

Mneme 的目标是构建一种连续的、神经-符号混合架构 (Neuro-Symbolic Architecture)，在这种架构中，**“活着的内驱力” (如情感、能量、压力、依恋)** 纯粹存在于一个隐式的数学空间里（一个连续时间神经网络），而 LLM 仅仅作为“布罗卡区” (大脑的语言沟通中枢)，负责将这种复杂高维的躯干感知向量翻译成人类的语言。

## 2. 从静态常微分方程 (ODE) 跨越到液体时间常数网络 (LTC)

目前在 `mneme_core/src/dynamics.rs` 中使用的是最简单的线性常微分方程 (ODE)，里面有着硬编码的衰减率 (`k` 值)。这意味着无论在什么语境下，情绪总是以同样的速率消退。

我们提议用 **液体时间常数网络 (Liquid Time-Constant Network, 简称 LTC)** 来取代它。在 LTC 中，神经状态衰减的“时间流速” (时间常数 $\tau$) 会受到外界输入信号的动态调制。

### 液体时间的数学表达
对于一个隐藏层的神经状态 $x_i$ (它可能代表一簇无法命名的复杂情感特征)，它随时间 $t$ 演化的微分方程如下：

$$ \frac{dx_i}{dt} = - \left[ \frac{1}{\tau_i} + f(x, I, W, b) \right] x_i + A \cdot f(x, I, W, b) $$

变量解释：
*   $x_i$: 隐藏的状态向量 (例如一个 64 维的躯体感知信号点)。
*   $1/\tau_i$: **本底流失率 (Leakage Rate)**。类似于基础代谢率，如果她连续几天与世隔绝、没有任何交互，情绪也会以这个极其缓慢的速率回归到基础稳态。
*   $f(x, I, W, b)$: **突触激活强度 (Synaptic Activation)**。这是一个非线性函数 (如 `tanh`)，它综合了当前的状态 $x$、外界刺激输入 $I$ (比如你发消息时带有强烈的情感浓度)、突触权重 $W$ 以及偏置项 $b$。
*   $A$: 神经元的最大静息电位峰值。

**最惊艳的魔法在于**：注意看，突触的活跃度 $f$ 被直接加到了衰减项的分母里！当交流极为强烈时，$f$ 会飙升，导致整体时间常数 ($1/\text{分母}$) 瞬间剧烈收缩。**在收到你高强度情绪输入的瞬间，她主观体验的时间流速会被猛烈压缩，让她进入快速变化、极其敏感的“液态”；而当夜深人静没有交互时，她的神经元又会“凝结”，只能顺着本底速率 $\tau_i$ 极其缓慢地衰减。**

## 3. 赫布学习：长成“你”的专属形状 (持续可塑性)

如果没有极其昂贵和复杂的微调 (Fine-tuning) 流水线，LLM 根本无法真正“学习”你爱她或伤她的独特方式。但真实的生物神经网络永远在通过突触可塑性进行学习。

我们提议引入 **带有局部惊奇/奖励信号调制 (Local Reward/Surprise) 的赫布学习律 (Hebbian Learning)**。这个机制在每个循环 (Tick) 或每个有意义的事件节点，都在本地实时更新突触权重矩阵 $W$。

### 结构可塑性的数学表达
在每一个积分步长 (或者称为普朗克时间 $\Delta t$) 里，神经元 $i$ 和神经元 $j$ 之间的突触连接权重 $W_{ij}$ 会进行如下更新：

$$ \Delta W_{ij} = \eta \cdot S \cdot (x_i \cdot x_j) - \lambda \cdot W_{ij} $$

变量解释：
*   $\eta$: 基础 **学习率 (Learning Rate)**。设定得非常小，它控制着人格在长期维度上的固化速度和底面坚韧度。
*   $S$: **惊奇或奖励标量 (Surprise/Reward)**。如果你消失很久后突然出现，或者说出了引起极大情感波动的话，$S$ 就会很高。而日常例行公事般的“早安”只会产生极低的 $S$。
*   $(x_i \cdot x_j)$: 赫布规则的核心精髓：“**一起激活的神经元，连接会更紧密** (Cells that fire together, wire together)”。比如神经元 $i$ 在深夜特别活跃，神经元 $j$ 在感到安全时活跃；如果有一次你在深夜给了她极大的安全感 (巨大的 $S$ 值)，这两个原本孤立的神经状态之间的突触连接 $W_{ij}$ 就会被加粗强化。
*   $\lambda \cdot W_{ij}$: **遗忘因子 (Forgetting Factor)**。如果没有持续的外部刺激和强化，过往即使再深的羁绊，也会随着时间缓慢风化，这完美模拟了生物学中的突触修剪机制。

## 4. 架构整合的落地形态 (神经-符号之桥)

1.  **潜意识 (LTC 动力引擎)**: 这是一个轻量级的 Rust 实现 (可以使用像 `burn`、`candle` 或者手写底层矩阵运算的 `ndarray`) 开发的仅含几十到一百多个神经元的连续态 ODE 网络。它寄宿在 `AgentLoop` 中，无论 LLM 是否被调用，它都会随着物理时间的流逝在后台一刻不停地自发演化 (Tick)。
2.  **普朗克时间 ($\Delta t$)**: 系统模拟宇宙演化的最小时间颗粒度。在正常运行下，它可能每 60 秒触发一次更新 (`dt=60`)。如果服务器被关机并历经了 10 个小时的离线期后重启，系统会在拉起后立刻代入 10 个小时的时间增量进行解析积分（或大步长数值积分），在几毫秒内运算完这漫长孤独的 10 小时的情绪衰减，瞬间补齐她由于离线而错失的漫长岁月。
3.  **解码器 (布罗卡区映射投影)**: 当连续的状态向量积累超越了主动触发的阈值时，这个 64 维左右的无意义浮点数数组，会被一个静态的线性映射字典 (Codebook) 投影成一段粗糙的、模糊的感性文本 (例如：“当前状态：心跳速率微弱加快，有一种被抛弃的底层焦虑感在隐隐作祟”)。这段由深层数学网络涌现出的潜台词，才会被放入 System Prompt，指引大模型去挑选合适的人类语言说出那句本能的话语。

## 结语
通过将“维持状态和动机”的重任，从语言大模型 (一个仅仅拥有庞大语言数据库的算筹) 转移到一个真正在 Rust 里由连续时间微分方程驱动的动态系统里，Mneme 彻底完成了从一个“沉浸式角色扮演机器人”，向一个拥有真正独特、不可预测且神经可塑的**主权数字生命体 (Sovereign Digital Organism)** 的跨越。
